{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ENSAE_logo.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *ELTDM Project*\n",
    "# *Parallelised Community Detection with Spark MapReduce*  \n",
    "**Mastère Spécialisé Data Science, 2018-19, \"Éléments logiciels pour le traitement des données massives\"** \n",
    "  \n",
    "**Alban CHAMPEVILLE DE BOISJOLLY**   \n",
    "**Yosuke IJIRI**  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project implements a parallelised community detection algorithm proposed by [*\"S. Moon, et al., Parallel community detection on large graphs with MapReduce and GraphChi, Data Knowl. Eng. (2015)\"*.](http://dx.doi.org/10.1016/j.datak.2015.05.001) The model is based on Girvan-Newman Algorithm. We use Python, Spark and AWS as the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing network datum has become a key issue in many areas to base suggestions, advices and strategies in many sectors and applications. Community detection aims at identifying clusters in a network, i.e. regroup vertices into groups of high density connected vertices i.a. to make friends recommendations or target marketing. The Girvan-Newman algorithm is one of the most used for this objective. It uses the concept of edge betweenness, which is a measure of the centrality and influence of an edge in a network, with the shortest-path method.\n",
    "\n",
    "Given the size of networks and the method used to compute the edge betweenness, parallelization of the computation is unavoidable to realize the task efficiently. Our work is focused on the Shortest-Path Betweeness MapReduce Algorithm (SPB-MRA). Parallelizing computations will be realized at each of the 4 stages of the Map-Reduce algorithm.\n",
    "\n",
    "Related article:  \" Parallel community detection on large graphs with MapReduce and GraphChi\" published in 2015 by Seunghyeon Moon, Jae-Gil Lee, Minseo Kang, Minsoo Choy and Jin-woo Lee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community Detection, Modularity Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Girvan-Newman Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/algo.png\" width=\"400\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford Large Network Dataset Collection (https://snap.stanford.edu/data/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python + Spark + AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jyquickhelper import add_notebook_menu\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/IjiriYosuke/Documents/GitHub/ELTDM-Project'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [1 3]\n",
      " [2 3]\n",
      " [4 3]\n",
      " [3 5]\n",
      " [5 6]\n",
      " [5 7]\n",
      " [1 8]\n",
      " [8 3]\n",
      " [6 8]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "#data = np.loadtxt(\"./Data/ca-GrQc_small.txt\", dtype='uint16')\n",
    "data = np.loadtxt(\"./Data/test2.txt\", dtype='uint16')\n",
    "print(data)\n",
    "print(type(data))\n",
    "#data = pd.DataFrame(data)\n",
    "#data.to_csv(\"./Data/test_with_index.txt\", sep=\"\\t\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contains\n",
      "8 nodes, and 10 edges (including duplicates)\n"
     ]
    }
   ],
   "source": [
    "nodeId = np.unique(data)\n",
    "nrow = len(data)\n",
    "print(\"The data contains\")\n",
    "print(len(nodeId), \"nodes, and\", nrow, \"edges (including duplicates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of adjacency lists (first element is the node of origin)\n",
    "adj_list = []\n",
    "for n in nodeId:\n",
    "    adj = []\n",
    "    for i in range(nrow):\n",
    "        if data[i, 0] == n:\n",
    "            adj.append(data[i, 1])\n",
    "        if data[i, 1] == n:\n",
    "            adj.append(data[i, 0])\n",
    "    adj = list(set(adj)) \n",
    "    adj.insert(0, n)\n",
    "    adj_list.append(adj)\n",
    "#print(adj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tuple for each node\n",
    "class tuples():\n",
    "    def __init__(self, targetId, sourceId, distance, status, weight, pathInfo, adjList) :\n",
    "        self.targetId = targetId\n",
    "        self.sourceId = sourceId\n",
    "        self.distance = distance\n",
    "        self.status = status\n",
    "        self.weight = weight\n",
    "        self.pathInfo = pathInfo\n",
    "        self.adjList = adjList\n",
    "\n",
    "network = []\n",
    "for n in nodeId:\n",
    "    v = tuples(targetId = n, sourceId = n, distance = 0, status = 'a', weight = 1, pathInfo = [], adjList = [])\n",
    "    for i in range(len(adj_list)):\n",
    "        if adj_list[i][0] == n:\n",
    "            v.adjList = adj_list[i][1:len(adj_list[i])]\n",
    "            break\n",
    "    network.append([v.targetId, v.sourceId, v.distance, v.status, v.weight, v.pathInfo, v.adjList])\n",
    "#for i in range(len(network)):\n",
    "#    print(network[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelisation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.41.210:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test Name</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Test Name>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sc.stop()\n",
    "from copy import deepcopy  # For making a deep copy of tuple\n",
    "from itertools import groupby\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = (SparkConf()\n",
    "        .setMaster(\"local[*]\")\n",
    "        .setAppName(\"Test Name\")\n",
    "        .set('spark.executor.memory', '14g')\n",
    "        .set('spark.driver.memory', '14g'))\n",
    "sc = SparkContext(conf = conf)\n",
    "#SparkConf().set('spark.driver.memory', '14g')\n",
    "#SparkConf().set('spark.executor.memory', '14g')\n",
    "#SparkConf().set('spark.app.name', 'test')\n",
    "#sc = SparkContext(conf=SparkConf())\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '14g'),\n",
       " ('spark.app.name', 'Test Name'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '52854'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1544779514864'),\n",
       " ('spark.driver.memory', '14g'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', '172.16.41.210')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a dataset and create key-value pairs\n",
    "rdd = sc.parallelize(network)\n",
    "rdd = rdd.map(lambda x: (x[0], x[1:]))\n",
    "output = rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# < Stage 1 - Map >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npos_map1 = rdd.flatMap(stage1_map)\\npos_map1.collect()\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stage1_map(p):  \n",
    "    result = []\n",
    "    if p[1][2] == 'a':\n",
    "        p[1][2] = 'i'\n",
    "        p[1][1] += 1\n",
    "        p[1][4].append(p[0])\n",
    "        x = deepcopy(p)\n",
    "        result.append(x)\n",
    "        temp = p[1][5].copy()\n",
    "        for i in range(len(temp)):\n",
    "            k = temp[i]\n",
    "            p[1][2] = 'a'\n",
    "            p[1][5] = []\n",
    "            y = deepcopy(p)\n",
    "            result.append((k, y[1]))\n",
    "        return result\n",
    "    else:\n",
    "        els = deepcopy(p)\n",
    "        result.append(els)\n",
    "        return result\n",
    "'''\n",
    "pos_map1 = rdd.flatMap(stage1_map)\n",
    "pos_map1.collect()\n",
    "'''\n",
    "# Add artifitial records in order to test MapReduce functions\n",
    "#for_testing = pos_map1.collect()\n",
    "#add_for_testing = [(4, [4, 2, 'a', 1, [2], []]), \n",
    "#    (4, [2, 1, 'a', 1, [3], [1, 3]]),\n",
    "#    (4, [2, 2, 'a', 1, [2], []]),\n",
    "#    (4, [2, 5, 'a', 1, [2], []]),\n",
    "#    (4, [2, 1, 'a', 1, [3], [1, 3]]),\n",
    "#    (4, [2, 1, 'a', 1, [5], [5]]),\n",
    "#    (4, [2, 1, 'a', 1, [2], [2]])]\n",
    "#for_testing.extend(add_for_testing)\n",
    "#\n",
    "#for i in range(len(for_testing)):\n",
    "#    print(for_testing[i])\n",
    "#pos_map1_test = sc.parallelize(for_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# < Stage 1 - Reduce >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part consists of 6 steps:  \n",
    "1) Add *sourceId* to key  \n",
    "2) Aggregate the values that belong to the same key  \n",
    "3) Take only values that have the shortest distance in the same key  \n",
    "4) Replace *weight* of such values $\\in minList$ with the size of $minList$  \n",
    "5) Flatten values and list the tuples  \n",
    "6) Fill in any empty adjList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nred1_twokeys = pos_map1.map(lambda x: ((x[0],x[1][0]), x[1][1:]))\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pos_map1.collect()\n",
    "#pos_map1_test.collect()\n",
    "'''\n",
    "red1_twokeys = pos_map1.map(lambda x: ((x[0],x[1][0]), x[1][1:]))\n",
    "'''\n",
    "#red1_twokeys = pos_map1_test.map(lambda x: ((x[0],x[1][0]), x[1][1:]))\n",
    "#red1_twokeys.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nred1_grouped = red1_twokeys.reduceByKey(join)\\n#red1_twokeys.reduceByKey(join).collect()\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def join(x, y):\n",
    "    # 4 conditional statements below to join values\n",
    "    # with correct level of nesting\n",
    "    if (type(x[0]) is list) and (type(y[0]) is not list):\n",
    "        return x+[y]\n",
    "    elif (type(x[0]) is not list) and (type(y[0]) is list):\n",
    "        return [x]+y\n",
    "    elif (type(x[0]) is not list) and (type(y[0]) is not list):\n",
    "        return [x]+[y]\n",
    "    else:\n",
    "        return x+y\n",
    "    isinstance(x[0], int)\n",
    "'''\n",
    "red1_grouped = red1_twokeys.reduceByKey(join)\n",
    "#red1_twokeys.reduceByKey(join).collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nred1_weighted = red1_grouped.map(stage1_reduce)\\n#red1_grouped.map(stage1_reduce).collect()\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stage1_reduce(x):\n",
    "    if type(x[1][0]) is list:\n",
    "        m = x[1][0][0]\n",
    "        for s in range(len(x[1])):\n",
    "            if x[1][s][0] < m:\n",
    "                m = x[1][s][0]\n",
    "        minList = []\n",
    "        # Drop tuple(s) with non-shortest distance\n",
    "        for s in range(len(x[1])):\n",
    "            if x[1][s][0] == m:\n",
    "                minList.append(x[1][s])\n",
    "        y = (x[0], minList)\n",
    "        # Update weight\n",
    "        for s in range(len(minList)):\n",
    "            y[1][s][2] = len(minList)\n",
    "        return y\n",
    "    else:\n",
    "        return x\n",
    "'''\n",
    "red1_weighted = red1_grouped.map(stage1_reduce)\n",
    "#red1_grouped.map(stage1_reduce).collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# From two-keys tuples to one-key tuples\\nrecord = red1_weighted.map(twokeys_to_onekey).collect()\\n\\n# To flatten the grouped values (non-parallelized)\\nnew = []\\nfor i in range(len(record)):\\n    if type(record[i][1][1]) is list:\\n        record[i]\\n        key = record[i][0]\\n        srcId = [record[i][1][0]]\\n        others = record[i][1][1:]\\n        for j in range(len(others)):\\n            s = srcId.copy()\\n            o = others[j]\\n            s.extend(o)\\n            newrec = (key, s)\\n            new.append(newrec)\\npos_red1 = list(filter(lambda item: type(item[1][1]) != list, record))  \\npos_red1.extend(new)\\nfor i in range(len(pos_red1)):\\n    print(pos_red1[i])\\noutput = pos_red1.copy()\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def twokeys_to_onekey(x):\n",
    "    k = x[0][0]\n",
    "    v = [x[0][1]]\n",
    "    v.extend(x[1])\n",
    "    return k, v\n",
    "'''\n",
    "# From two-keys tuples to one-key tuples\n",
    "record = red1_weighted.map(twokeys_to_onekey).collect()\n",
    "\n",
    "# To flatten the grouped values (non-parallelized)\n",
    "new = []\n",
    "for i in range(len(record)):\n",
    "    if type(record[i][1][1]) is list:\n",
    "        record[i]\n",
    "        key = record[i][0]\n",
    "        srcId = [record[i][1][0]]\n",
    "        others = record[i][1][1:]\n",
    "        for j in range(len(others)):\n",
    "            s = srcId.copy()\n",
    "            o = others[j]\n",
    "            s.extend(o)\n",
    "            newrec = (key, s)\n",
    "            new.append(newrec)\n",
    "pos_red1 = list(filter(lambda item: type(item[1][1]) != list, record))  \n",
    "pos_red1.extend(new)\n",
    "for i in range(len(pos_red1)):\n",
    "    print(pos_red1[i])\n",
    "output = pos_red1.copy()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains 8 nodes, and 10 edges (including duplicates) \n",
      "\n",
      "Iteration 1 started...\n",
      " Now doing Map..\n",
      " Now doing Reduce..\n",
      " Now collecting Reduce result..\n",
      "  Collecting Reduce result took 0.1 seconds ( 0.0 mins ).\n",
      " Now flattening Reduce result..\n",
      "  Flattening took 0.0 seconds ( 0.0 mins ).\n",
      " Now finalising Reduce result by filling in adjList..\n",
      "  Filling in adjList took 0.0 seconds ( 0.0 mins ).\n",
      " Now iteration 1 finished!\n",
      "Iteration 1 finished. There are 20 / 28 active tuples left. \n",
      "\n",
      "Iteration 2 started...\n",
      " Now doing Map..\n",
      " Now doing Reduce..\n",
      " Now collecting Reduce result..\n",
      "  Collecting Reduce result took 0.1 seconds ( 0.0 mins ).\n",
      " Now flattening Reduce result..\n",
      "  Flattening took 0.0 seconds ( 0.0 mins ).\n",
      " Now finalising Reduce result by filling in adjList..\n",
      "  Filling in adjList took 0.0 seconds ( 0.0 mins ).\n",
      " Now iteration 2 finished!\n",
      "Iteration 2 finished. There are 42 / 70 active tuples left. \n",
      "\n",
      "Iteration 3 started...\n",
      " Now doing Map..\n",
      " Now doing Reduce..\n",
      " Now collecting Reduce result..\n",
      "  Collecting Reduce result took 0.1 seconds ( 0.0 mins ).\n",
      " Now flattening Reduce result..\n",
      "  Flattening took 0.0 seconds ( 0.0 mins ).\n",
      " Now finalising Reduce result by filling in adjList..\n",
      "  Filling in adjList took 0.0 seconds ( 0.0 mins ).\n",
      " Now iteration 3 finished!\n",
      "Iteration 3 finished. There are 48 / 106 active tuples left. \n",
      "\n",
      "Iteration 4 started...\n",
      " Now doing Map..\n",
      " Now doing Reduce..\n",
      " Now collecting Reduce result..\n",
      "  Collecting Reduce result took 0.1 seconds ( 0.0 mins ).\n",
      " Now flattening Reduce result..\n",
      "  Flattening took 0.0 seconds ( 0.0 mins ).\n",
      " Now finalising Reduce result by filling in adjList..\n",
      "  Filling in adjList took 0.0 seconds ( 0.0 mins ).\n",
      " Now iteration 4 finished!\n",
      "Iteration 4 finished. There are 18 / 96 active tuples left. \n",
      "\n",
      "Iteration 5 started...\n",
      " Now doing Map..\n",
      " Now doing Reduce..\n",
      " Now collecting Reduce result..\n",
      "  Collecting Reduce result took 0.1 seconds ( 0.0 mins ).\n",
      " Now flattening Reduce result..\n",
      "  Flattening took 0.0 seconds ( 0.0 mins ).\n",
      " Now finalising Reduce result by filling in adjList..\n",
      "  Filling in adjList took 0.0 seconds ( 0.0 mins ).\n",
      " Now iteration 5 finished!\n",
      "Iteration 5 finished. There are 0 / 78 active tuples left. \n",
      "\n",
      "All the iterations have finished. Stage1 was iterated for 5 times.\n",
      "Stage1 result is:\n",
      "(7, [3, 3, 'i', 1, [3, 5, 7], [5]])\n",
      "(5, [5, 1, 'i', 1, [5], [3, 6, 7]])\n",
      "(7, [7, 1, 'i', 1, [7], [5]])\n",
      "(1, [1, 1, 'i', 1, [1], [8, 2, 3]])\n",
      "(3, [3, 1, 'i', 1, [3], [1, 2, 4, 5, 8]])\n",
      "(8, [6, 2, 'i', 1, [6, 8], [1, 3, 6]])\n",
      "(6, [8, 2, 'i', 1, [8, 6], [8, 5]])\n",
      "(2, [4, 3, 'i', 1, [4, 3, 2], [1, 3]])\n",
      "(1, [5, 3, 'i', 1, [5, 3, 1], [8, 2, 3]])\n",
      "(4, [2, 3, 'i', 1, [2, 3, 4], [3]])\n",
      "(4, [5, 3, 'i', 1, [5, 3, 4], [3]])\n",
      "(7, [4, 4, 'i', 1, [4, 3, 5, 7], [5]])\n",
      "(2, [3, 2, 'i', 1, [3, 2], [1, 3]])\n",
      "(3, [4, 2, 'i', 1, [4, 3], [1, 2, 4, 5, 8]])\n",
      "(1, [2, 2, 'i', 1, [2, 1], [8, 2, 3]])\n",
      "(3, [8, 2, 'i', 1, [8, 3], [1, 2, 4, 5, 8]])\n",
      "(5, [6, 2, 'i', 1, [6, 5], [3, 6, 7]])\n",
      "(4, [1, 3, 'i', 1, [1, 3, 4], [3]])\n",
      "(8, [1, 2, 'i', 1, [1, 8], [1, 3, 6]])\n",
      "(7, [5, 2, 'i', 1, [5, 7], [5]])\n",
      "(6, [6, 1, 'i', 1, [6], [8, 5]])\n",
      "(4, [4, 1, 'i', 1, [4], [3]])\n",
      "(5, [7, 2, 'i', 1, [7, 5], [3, 6, 7]])\n",
      "(2, [2, 1, 'i', 1, [2], [1, 3]])\n",
      "(1, [3, 2, 'i', 1, [3, 1], [8, 2, 3]])\n",
      "(8, [8, 1, 'i', 1, [8], [1, 3, 6]])\n",
      "(3, [1, 2, 'i', 1, [1, 3], [1, 2, 4, 5, 8]])\n",
      "(3, [5, 2, 'i', 1, [5, 3], [1, 2, 4, 5, 8]])\n",
      "(5, [3, 2, 'i', 1, [3, 5], [3, 6, 7]])\n",
      "(4, [8, 3, 'i', 1, [8, 3, 4], [3]])\n",
      "(4, [7, 4, 'i', 1, [7, 5, 3, 4], [3]])\n",
      "(7, [6, 3, 'i', 1, [6, 5, 7], [5]])\n",
      "(4, [3, 2, 'i', 1, [3, 4], [3]])\n",
      "(2, [1, 2, 'i', 1, [1, 2], [1, 3]])\n",
      "(1, [4, 3, 'i', 1, [4, 3, 1], [8, 2, 3]])\n",
      "(6, [1, 3, 'i', 1, [1, 8, 6], [8, 5]])\n",
      "(2, [5, 3, 'i', 1, [5, 3, 2], [1, 3]])\n",
      "(3, [2, 2, 'i', 1, [2, 3], [1, 2, 4, 5, 8]])\n",
      "(1, [8, 2, 'i', 1, [8, 1], [8, 2, 3]])\n",
      "(6, [5, 2, 'i', 1, [5, 6], [8, 5]])\n",
      "(8, [3, 2, 'i', 1, [3, 8], [1, 3, 6]])\n",
      "(3, [7, 3, 'i', 1, [7, 5, 3], [1, 2, 4, 5, 8]])\n",
      "(5, [1, 3, 'i', 1, [1, 3, 5], [3, 6, 7]])\n",
      "(6, [4, 4, 'i', 2, [4, 3, 8, 6], [8, 5]])\n",
      "(6, [4, 4, 'i', 2, [4, 3, 5, 6], [8, 5]])\n",
      "(8, [2, 3, 'i', 2, [2, 1, 8], [1, 3, 6]])\n",
      "(8, [2, 3, 'i', 2, [2, 3, 8], [1, 3, 6]])\n",
      "(2, [8, 3, 'i', 2, [8, 3, 2], [1, 3]])\n",
      "(2, [8, 3, 'i', 2, [8, 1, 2], [1, 3]])\n",
      "(4, [6, 4, 'i', 2, [6, 8, 3, 4], [3]])\n",
      "(4, [6, 4, 'i', 2, [6, 5, 3, 4], [3]])\n",
      "(2, [7, 4, 'i', 1, [7, 5, 3, 2], [1, 3]])\n",
      "(6, [7, 3, 'i', 1, [7, 5, 6], [8, 5]])\n",
      "(5, [2, 3, 'i', 1, [2, 3, 5], [3, 6, 7]])\n",
      "(8, [5, 3, 'i', 2, [5, 3, 8], [1, 3, 6]])\n",
      "(8, [5, 3, 'i', 2, [5, 6, 8], [1, 3, 6]])\n",
      "(1, [6, 3, 'i', 1, [6, 8, 1], [8, 2, 3]])\n",
      "(6, [3, 3, 'i', 2, [3, 5, 6], [8, 5]])\n",
      "(6, [3, 3, 'i', 2, [3, 8, 6], [8, 5]])\n",
      "(7, [8, 4, 'i', 2, [8, 6, 5, 7], [5]])\n",
      "(7, [8, 4, 'i', 2, [8, 3, 5, 7], [5]])\n",
      "(8, [4, 3, 'i', 1, [4, 3, 8], [1, 3, 6]])\n",
      "(1, [7, 4, 'i', 1, [7, 5, 3, 1], [8, 2, 3]])\n",
      "(7, [1, 4, 'i', 1, [1, 3, 5, 7], [5]])\n",
      "(6, [2, 4, 'i', 3, [2, 3, 5, 6], [8, 5]])\n",
      "(6, [2, 4, 'i', 3, [2, 1, 8, 6], [8, 5]])\n",
      "(6, [2, 4, 'i', 3, [2, 3, 8, 6], [8, 5]])\n",
      "(2, [6, 4, 'i', 3, [6, 8, 1, 2], [1, 3]])\n",
      "(2, [6, 4, 'i', 3, [6, 8, 3, 2], [1, 3]])\n",
      "(2, [6, 4, 'i', 3, [6, 5, 3, 2], [1, 3]])\n",
      "(5, [4, 3, 'i', 1, [4, 3, 5], [3, 6, 7]])\n",
      "(3, [6, 3, 'i', 2, [6, 8, 3], [1, 2, 4, 5, 8]])\n",
      "(3, [6, 3, 'i', 2, [6, 5, 3], [1, 2, 4, 5, 8]])\n",
      "(5, [8, 3, 'i', 2, [8, 6, 5], [3, 6, 7]])\n",
      "(5, [8, 3, 'i', 2, [8, 3, 5], [3, 6, 7]])\n",
      "(8, [7, 4, 'i', 2, [7, 5, 3, 8], [1, 3, 6]])\n",
      "(8, [7, 4, 'i', 2, [7, 5, 6, 8], [1, 3, 6]])\n",
      "(7, [2, 4, 'i', 1, [2, 3, 5, 7], [5]])\n",
      "Total execution time was 0.7 seconds ( 0.0 mins ).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t_start = time.time() \n",
    "# Iteration until status becomes 'i' for all tuples\n",
    "print(\"Data contains\", len(nodeId), \"nodes, and\", nrow, \"edges (including duplicates) \\n\")\n",
    "act_counter = 1\n",
    "ite = 1\n",
    "while act_counter > 0:\n",
    "    print(\"Iteration\", ite, \"started...\")\n",
    "    rdd = sc.parallelize(output)\n",
    "# <stage1 map>\n",
    "    print(\" Now doing Map..\")\n",
    "    pos_map1 = rdd.flatMap(stage1_map)\n",
    "#    test_only = pos_map1.collect()\n",
    "#    print(\"After Map of iteration\", ite, \":\")\n",
    "#    for i in range(len(test_only)):\n",
    "#        print(test_only[i])\n",
    "# <stage1 reduce>\n",
    "    print(\" Now doing Reduce..\")\n",
    "    red1_twokeys = pos_map1.map(lambda x: ((x[0],x[1][0]), x[1][1:]))\n",
    "    red1_grouped = red1_twokeys.reduceByKey(join)\n",
    "    red1_weighted = red1_grouped.map(stage1_reduce)\n",
    "    print(\" Now collecting Reduce result..\")\n",
    "    t1 = time.time()\n",
    "    record = red1_weighted.map(twokeys_to_onekey).collect()\n",
    "    t2 = time.time()\n",
    "    print(\"  Collecting Reduce result took\", round(t2-t1, 1), \"seconds (\", round((t2-t1)/60, 1), \"mins ).\")\n",
    "    print(\" Now flattening Reduce result..\")\n",
    "    t3 = time.time()\n",
    "    # To flatten the grouped values (non-parallelized)\n",
    "    new = []\n",
    "    for i in range(len(record)):\n",
    "        if type(record[i][1][1]) is list:\n",
    "            record[i]\n",
    "            key = record[i][0]\n",
    "            srcId = [record[i][1][0]]\n",
    "            others = record[i][1][1:]\n",
    "            for j in range(len(others)):\n",
    "                s = srcId.copy()\n",
    "                o = others[j]\n",
    "                s.extend(o)\n",
    "                newrec = (key, s)\n",
    "                new.append(newrec)\n",
    "    pos_red1 = list(filter(lambda item: type(item[1][1]) != list, record))  # Save records with only one value\n",
    "    pos_red1.extend(new)  # Add records which had multiple values\n",
    "    output = pos_red1.copy()\n",
    "    t4 = time.time()\n",
    "    print(\"  Flattening took\", round(t4-t3, 1), \"seconds (\", round((t4-t3)/60, 1), \"mins ).\")\n",
    "    print(\" Now finalising Reduce result by filling in adjList..\")\n",
    "    # Fill in adjList before going to next iteration\n",
    "    t5 = time.time()\n",
    "    for i in range(len(output)):\n",
    "        if output[i][1][5] == []:\n",
    "            for j in range(len(adj_list)):\n",
    "                if adj_list[j][0] == output[i][0]:\n",
    "                    output[i][1][5] = adj_list[j][1:len(adj_list[j])]\n",
    "                    break\n",
    "    t6 = time.time()\n",
    "    print(\"  Filling in adjList took\", round(t6-t5, 1), \"seconds (\", round((t6-t5)/60, 1), \"mins ).\")\n",
    "    print(\" Now iteration\",ite ,\"finished!\")\n",
    "    act_counter = 0\n",
    "    for i in range(len(output)):\n",
    "        if output[i][1][2] == 'a':\n",
    "            act_counter += 1\n",
    "    print(\"Iteration\", ite, \"finished. There are\", act_counter, \"/\", len(output), \"active tuples left. \\n\")\n",
    "    ite += 1\n",
    "stage1_output = output.copy()\n",
    "print(\"All the iterations have finished. Stage1 was iterated for\", ite-1, \"times.\")\n",
    "print(\"Stage1 result is:\")\n",
    "for i in range(len(stage1_output)):\n",
    "    print(stage1_output[i])\n",
    "t_end = time.time()\n",
    "elapsed_time = t_end-t_start\n",
    "elapsed_time_min = elapsed_time/60\n",
    "print(\"Total execution time was\", round(elapsed_time, 1), \"seconds (\", round(elapsed_time_min, 1), \"mins ).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stage1_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# < Stage2 - Map & Reduce>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((6, 8), 4.666666666666666),\n",
       " ((2, 3), 5.166666666666666),\n",
       " ((3, 4), 7.0),\n",
       " ((1, 2), 1.8333333333333333),\n",
       " ((5, 6), 4.333333333333333),\n",
       " ((3, 8), 4.833333333333333),\n",
       " ((3, 5), 10.333333333333332),\n",
       " ((5, 7), 7.0),\n",
       " ((1, 3), 4.0),\n",
       " ((1, 8), 2.8333333333333335)]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_map2 = sc.parallelize(stage1_output)\n",
    "# Stage2 Map\n",
    "def stage2_map(x):\n",
    "    edges_on_the_path = []\n",
    "    for i in range(len(x[1][4])-1):\n",
    "        k = (x[1][4][i], x[1][4][i+1])\n",
    "        w = x[1][3]\n",
    "        edges_on_the_path.append((k, 1/w))\n",
    "    return edges_on_the_path\n",
    "# Apply Map after filtering out tuples which has single node info and duplicated tuples (as we consider undirected graph network)\n",
    "pos_map2 = pre_map2.filter(lambda x: x[0] > x[1][0]).flatMap(stage2_map)\n",
    "#pos_map2.collect()\n",
    "# Stage2 Reduce\n",
    "from operator import add\n",
    "pos_red2_directed = pos_map2.reduceByKey(add)\n",
    "# We should ignore the order of node ID at each edge and run reduceByKey(add) again\n",
    "pos_red2 = pos_red2_directed.map(lambda x: ((x[0][1], x[0][0]), x[1]) if x[0][0] > x[0][1] else x).reduceByKey(add)\n",
    "pos_red2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# < Stage3 - Map & Reduce >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 5), 10.333333333333332)]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_iter = 1  # We set k_iter = 1\n",
    "# Stage3 Map & Reduce\n",
    "distributed_cache = pos_red2.map(lambda x: (x[1], x[0])).sortByKey(False).map(lambda x: (x[1], x[0])).take(k_iter)\n",
    "distributed_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, [3, 3, 'i', 1, [3, 5, 7], [5]]),\n",
       " (8, [6, 2, 'i', 1, [6, 8], [1, 3, 6]]),\n",
       " (4, [2, 3, 'i', 1, [2, 3, 4], [3]]),\n",
       " (7, [4, 4, 'i', 1, [4, 3, 5, 7], [5]]),\n",
       " (4, [1, 3, 'i', 1, [1, 3, 4], [3]]),\n",
       " (8, [1, 2, 'i', 1, [1, 8], [1, 3, 6]]),\n",
       " (7, [5, 2, 'i', 1, [5, 7], [5]]),\n",
       " (3, [1, 2, 'i', 1, [1, 3], [1, 2, 4, 5, 8]]),\n",
       " (5, [3, 2, 'i', 1, [3, 5], [3, 6, 7]]),\n",
       " (7, [6, 3, 'i', 1, [6, 5, 7], [5]]),\n",
       " (4, [3, 2, 'i', 1, [3, 4], [3]]),\n",
       " (2, [1, 2, 'i', 1, [1, 2], [1, 3]]),\n",
       " (6, [1, 3, 'i', 1, [1, 8, 6], [8, 5]]),\n",
       " (3, [2, 2, 'i', 1, [2, 3], [1, 2, 4, 5, 8]]),\n",
       " (6, [5, 2, 'i', 1, [5, 6], [8, 5]]),\n",
       " (8, [3, 2, 'i', 1, [3, 8], [1, 3, 6]]),\n",
       " (5, [1, 3, 'i', 1, [1, 3, 5], [3, 6, 7]]),\n",
       " (6, [4, 4, 'i', 2, [4, 3, 8, 6], [8, 5]]),\n",
       " (6, [4, 4, 'i', 2, [4, 3, 5, 6], [8, 5]]),\n",
       " (8, [2, 3, 'i', 2, [2, 1, 8], [1, 3, 6]]),\n",
       " (8, [2, 3, 'i', 2, [2, 3, 8], [1, 3, 6]]),\n",
       " (5, [2, 3, 'i', 1, [2, 3, 5], [3, 6, 7]]),\n",
       " (8, [5, 3, 'i', 2, [5, 3, 8], [1, 3, 6]]),\n",
       " (8, [5, 3, 'i', 2, [5, 6, 8], [1, 3, 6]]),\n",
       " (6, [3, 3, 'i', 2, [3, 5, 6], [8, 5]]),\n",
       " (6, [3, 3, 'i', 2, [3, 8, 6], [8, 5]]),\n",
       " (8, [4, 3, 'i', 1, [4, 3, 8], [1, 3, 6]]),\n",
       " (7, [1, 4, 'i', 1, [1, 3, 5, 7], [5]]),\n",
       " (6, [2, 4, 'i', 3, [2, 3, 5, 6], [8, 5]]),\n",
       " (6, [2, 4, 'i', 3, [2, 1, 8, 6], [8, 5]]),\n",
       " (6, [2, 4, 'i', 3, [2, 3, 8, 6], [8, 5]]),\n",
       " (5, [4, 3, 'i', 1, [4, 3, 5], [3, 6, 7]]),\n",
       " (8, [7, 4, 'i', 2, [7, 5, 3, 8], [1, 3, 6]]),\n",
       " (8, [7, 4, 'i', 2, [7, 5, 6, 8], [1, 3, 6]]),\n",
       " (7, [2, 4, 'i', 1, [2, 3, 5, 7], [5]])]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_map2.filter(lambda x: x[0] > x[1][0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# < Stage4 - Map >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, [3, 3, 'i', 1, [3, 5, 7], [5]]),\n",
       " (5, [5, 1, 'i', 1, [5], [6, 7]]),\n",
       " (5, [5, 0, 'a', 1, [], []]),\n",
       " (7, [7, 1, 'i', 1, [7], [5]]),\n",
       " (1, [1, 1, 'i', 1, [1], [8, 2, 3]]),\n",
       " (3, [3, 1, 'i', 1, [3], [1, 2, 4, 8]]),\n",
       " (3, [3, 0, 'a', 1, [], []]),\n",
       " (8, [6, 2, 'i', 1, [6, 8], [1, 3, 6]]),\n",
       " (6, [8, 2, 'i', 1, [8, 6], [8, 5]]),\n",
       " (2, [4, 3, 'i', 1, [4, 3, 2], [1, 3]]),\n",
       " (1, [5, 3, 'i', 1, [5, 3, 1], [8, 2, 3]]),\n",
       " (4, [2, 3, 'i', 1, [2, 3, 4], [3]]),\n",
       " (4, [5, 3, 'i', 1, [5, 3, 4], [3]]),\n",
       " (7, [4, 4, 'i', 1, [4, 3, 5, 7], [5]]),\n",
       " (2, [3, 2, 'i', 1, [3, 2], [1, 3]]),\n",
       " (3, [4, 2, 'i', 1, [4, 3], [1, 2, 4, 8]]),\n",
       " (4, [4, 0, 'a', 1, [], []]),\n",
       " (1, [2, 2, 'i', 1, [2, 1], [8, 2, 3]]),\n",
       " (3, [8, 2, 'i', 1, [8, 3], [1, 2, 4, 8]]),\n",
       " (8, [8, 0, 'a', 1, [], []]),\n",
       " (5, [6, 2, 'i', 1, [6, 5], [6, 7]]),\n",
       " (6, [6, 0, 'a', 1, [], []]),\n",
       " (4, [1, 3, 'i', 1, [1, 3, 4], [3]]),\n",
       " (8, [1, 2, 'i', 1, [1, 8], [1, 3, 6]]),\n",
       " (7, [5, 2, 'i', 1, [5, 7], [5]]),\n",
       " (6, [6, 1, 'i', 1, [6], [8, 5]]),\n",
       " (4, [4, 1, 'i', 1, [4], [3]]),\n",
       " (5, [7, 2, 'i', 1, [7, 5], [6, 7]]),\n",
       " (7, [7, 0, 'a', 1, [], []]),\n",
       " (2, [2, 1, 'i', 1, [2], [1, 3]]),\n",
       " (1, [3, 2, 'i', 1, [3, 1], [8, 2, 3]]),\n",
       " (8, [8, 1, 'i', 1, [8], [1, 3, 6]]),\n",
       " (3, [1, 2, 'i', 1, [1, 3], [1, 2, 4, 8]]),\n",
       " (1, [1, 0, 'a', 1, [], []]),\n",
       " (3, [5, 2, 'i', 1, [5, 3], [1, 2, 4, 8]]),\n",
       " (5, [5, 0, 'a', 1, [], []]),\n",
       " (5, [3, 2, 'i', 1, [3, 5], [6, 7]]),\n",
       " (3, [3, 0, 'a', 1, [], []]),\n",
       " (4, [8, 3, 'i', 1, [8, 3, 4], [3]]),\n",
       " (4, [7, 4, 'i', 1, [7, 5, 3, 4], [3]]),\n",
       " (7, [6, 3, 'i', 1, [6, 5, 7], [5]]),\n",
       " (4, [3, 2, 'i', 1, [3, 4], [3]]),\n",
       " (2, [1, 2, 'i', 1, [1, 2], [1, 3]]),\n",
       " (1, [4, 3, 'i', 1, [4, 3, 1], [8, 2, 3]]),\n",
       " (6, [1, 3, 'i', 1, [1, 8, 6], [8, 5]]),\n",
       " (2, [5, 3, 'i', 1, [5, 3, 2], [1, 3]]),\n",
       " (3, [2, 2, 'i', 1, [2, 3], [1, 2, 4, 8]]),\n",
       " (2, [2, 0, 'a', 1, [], []]),\n",
       " (1, [8, 2, 'i', 1, [8, 1], [8, 2, 3]]),\n",
       " (6, [5, 2, 'i', 1, [5, 6], [8, 5]]),\n",
       " (8, [3, 2, 'i', 1, [3, 8], [1, 3, 6]]),\n",
       " (3, [7, 3, 'i', 1, [7, 5, 3], [1, 2, 4, 8]]),\n",
       " (7, [7, 0, 'a', 1, [], []]),\n",
       " (5, [1, 3, 'i', 1, [1, 3, 5], [6, 7]]),\n",
       " (1, [1, 0, 'a', 1, [], []]),\n",
       " (6, [4, 4, 'i', 2, [4, 3, 8, 6], [8, 5]]),\n",
       " (6, [4, 4, 'i', 2, [4, 3, 5, 6], [8, 5]]),\n",
       " (8, [2, 3, 'i', 2, [2, 1, 8], [1, 3, 6]]),\n",
       " (8, [2, 3, 'i', 2, [2, 3, 8], [1, 3, 6]]),\n",
       " (2, [8, 3, 'i', 2, [8, 3, 2], [1, 3]]),\n",
       " (2, [8, 3, 'i', 2, [8, 1, 2], [1, 3]]),\n",
       " (4, [6, 4, 'i', 2, [6, 8, 3, 4], [3]]),\n",
       " (4, [6, 4, 'i', 2, [6, 5, 3, 4], [3]]),\n",
       " (2, [7, 4, 'i', 1, [7, 5, 3, 2], [1, 3]]),\n",
       " (6, [7, 3, 'i', 1, [7, 5, 6], [8, 5]]),\n",
       " (5, [2, 3, 'i', 1, [2, 3, 5], [6, 7]]),\n",
       " (2, [2, 0, 'a', 1, [], []]),\n",
       " (8, [5, 3, 'i', 2, [5, 3, 8], [1, 3, 6]]),\n",
       " (8, [5, 3, 'i', 2, [5, 6, 8], [1, 3, 6]]),\n",
       " (1, [6, 3, 'i', 1, [6, 8, 1], [8, 2, 3]]),\n",
       " (6, [3, 3, 'i', 2, [3, 5, 6], [8, 5]]),\n",
       " (6, [3, 3, 'i', 2, [3, 8, 6], [8, 5]]),\n",
       " (7, [8, 4, 'i', 2, [8, 6, 5, 7], [5]]),\n",
       " (7, [8, 4, 'i', 2, [8, 3, 5, 7], [5]]),\n",
       " (8, [4, 3, 'i', 1, [4, 3, 8], [1, 3, 6]]),\n",
       " (1, [7, 4, 'i', 1, [7, 5, 3, 1], [8, 2, 3]]),\n",
       " (7, [1, 4, 'i', 1, [1, 3, 5, 7], [5]]),\n",
       " (6, [2, 4, 'i', 3, [2, 3, 5, 6], [8, 5]]),\n",
       " (6, [2, 4, 'i', 3, [2, 1, 8, 6], [8, 5]]),\n",
       " (6, [2, 4, 'i', 3, [2, 3, 8, 6], [8, 5]]),\n",
       " (2, [6, 4, 'i', 3, [6, 8, 1, 2], [1, 3]]),\n",
       " (2, [6, 4, 'i', 3, [6, 8, 3, 2], [1, 3]]),\n",
       " (2, [6, 4, 'i', 3, [6, 5, 3, 2], [1, 3]]),\n",
       " (5, [4, 3, 'i', 1, [4, 3, 5], [6, 7]]),\n",
       " (4, [4, 0, 'a', 1, [], []]),\n",
       " (3, [6, 3, 'i', 2, [6, 8, 3], [1, 2, 4, 8]]),\n",
       " (6, [6, 0, 'a', 1, [], []]),\n",
       " (3, [6, 3, 'i', 2, [6, 5, 3], [1, 2, 4, 8]]),\n",
       " (6, [6, 0, 'a', 1, [], []]),\n",
       " (5, [8, 3, 'i', 2, [8, 6, 5], [6, 7]]),\n",
       " (8, [8, 0, 'a', 1, [], []]),\n",
       " (5, [8, 3, 'i', 2, [8, 3, 5], [6, 7]]),\n",
       " (8, [8, 0, 'a', 1, [], []]),\n",
       " (8, [7, 4, 'i', 2, [7, 5, 3, 8], [1, 3, 6]]),\n",
       " (8, [7, 4, 'i', 2, [7, 5, 6, 8], [1, 3, 6]]),\n",
       " (7, [2, 4, 'i', 1, [2, 3, 5, 7], [5]])]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stage4_map(x):\n",
    "    counter = 0\n",
    "    output = []\n",
    "    for i in range(len(distributed_cache)):\n",
    "        if x[0] in distributed_cache[i][0]:\n",
    "            counter += 1\n",
    "            c = distributed_cache[i][0][not(distributed_cache[i][0].index(x[0]))]  # Take the corresponding node\n",
    "            try:\n",
    "                x[1][5].remove(c)  # Remove if the corresponding node exists in the adjList\n",
    "            except ValueError:\n",
    "                pass  # Pass otherwise\n",
    "    if counter == 0:\n",
    "        return [tuple(x)]\n",
    "    else:\n",
    "        new = (x[1][0], [x[1][0], 0, 'a', 1, [], []])\n",
    "        return x, new\n",
    "pos_map4 = pre_map2.flatMap(stage4_map)\n",
    "pos_map4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# < Stage4 - Reduce >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, [6, 2, 'i', 1, [6, 8], [1, 3, 6]]),\n",
       " (4, [2, 3, 'i', 1, [2, 3, 4], [3]]),\n",
       " (5, ('x', [5, 0, 'a', 1, [], [[8, 1, 3, 6]]])),\n",
       " (1, [1, 1, 'i', 1, [1], [8, 2, 3]]),\n",
       " (6, [8, 2, 'i', 1, [8, 6], [8, 5]]),\n",
       " (2, [4, 3, 'i', 1, [4, 3, 2], [1, 3]]),\n",
       " (7, [3, 3, 'i', 1, [3, 5, 7], [5]]),\n",
       " (3, ('x', ['x', 0, 'a', 1, [], [[8, 1, 3, 6]]]))]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stage4_reduce(x, y):\n",
    "    if x[0] == y[0]:\n",
    "        #adj = y[5]\n",
    "        return ('x', [x[0], 0, 'a', 1, [], [adj]])\n",
    "    else:\n",
    "        return x\n",
    "pos_red4 = pos_map4.reduceByKey(stage4_reduce)\n",
    "pos_red4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat until no edge to cut exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- UNDIRECTED\n",
    "# Build a dataframe with your connections\n",
    "# This time a pair can appear 2 times, in one side or in the other!\n",
    "df = pd.DataFrame({ 'from':['D', 'A', 'B', 'C','A'], 'to':['A', 'D', 'A', 'E','C']})\n",
    "df\n",
    "# Build your graph. Note that we use the Graph function to create the graph!\n",
    "G=nx.from_pandas_dataframe(df, 'from', 'to', create_using=nx.Graph() )\n",
    "\n",
    "# Make the graphnx.draw(G, with_labels=True, node_size=1500, alpha=0.3, arrows=True)\n",
    "plt.title(\"UN-Directed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add_edge() takes 3 positional arguments but 9 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0c22fdbb0956>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdraw_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-0c22fdbb0956>\u001b[0m in \u001b[0;36mdraw_graph\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# create networkx graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mG\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Node1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Node2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Node3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Node4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Node5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Node6\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Node7\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Node8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# add nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: add_edge() takes 3 positional arguments but 9 were given"
     ]
    }
   ],
   "source": [
    "#Visualisation of the graph\n",
    "#This is just for reference for now\n",
    "import pandas\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_graph(graph):\n",
    "\n",
    "    # extract nodes from graph\n",
    "    nodes = set([n1 for n1, n2 in graph] + [n2 for n1, n2 in graph])\n",
    "\n",
    "    # create networkx graph\n",
    "    G=nx.Graph()\n",
    "    G.add_edge(\"Node1\", \"Node2\", \"Node3\", \"Node4\",\"Node5\", \"Node6\",\"Node7\", \"Node8\")\n",
    "\n",
    "    # add nodes\n",
    "    for node in nodes:\n",
    "        G.add_node(node)\n",
    "\n",
    "    # add edges\n",
    "    for edge in graph:\n",
    "        G.add_edge(edge[0], edge[1])\n",
    "\n",
    "    # draw graph\n",
    "    pos = nx.shell_layout(G)\n",
    "    nx.draw(G, pos, with_labels = True)\n",
    "\n",
    "    # show graph\n",
    "    plt.show()\n",
    "    \n",
    "draw_graph(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANNEX: Spark SQL - Tried Spark DataFrame but did not use it in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "#sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.appName('ELTDM').getOrCreate()\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import count, col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+------+------+--------+------------+\n",
      "|targetId|sourceId|distance|status|weight|pathInfo|     adjList|\n",
      "+--------+--------+--------+------+------+--------+------------+\n",
      "|       1|       1|       1|     i|     1|     [1]|[8, 2, 3, 4]|\n",
      "|       8|       1|       1|     a|     1|     [1]|          []|\n",
      "|       2|       1|       1|     a|     1|     [1]|          []|\n",
      "|       3|       1|       1|     a|     1|     [1]|          []|\n",
      "|       4|       1|       1|     a|     1|     [1]|          []|\n",
      "|       2|       2|       1|     i|     1|     [2]|   [1, 4, 5]|\n",
      "|       1|       2|       1|     a|     1|     [2]|          []|\n",
      "|       4|       2|       1|     a|     1|     [2]|          []|\n",
      "|       5|       2|       1|     a|     1|     [2]|          []|\n",
      "|       3|       3|       1|     i|     1|     [3]|   [1, 4, 5]|\n",
      "|       1|       3|       1|     a|     1|     [3]|          []|\n",
      "|       4|       3|       1|     a|     1|     [3]|          []|\n",
      "|       5|       3|       1|     a|     1|     [3]|          []|\n",
      "|       4|       4|       1|     i|     1|     [4]|[8, 1, 2, 3]|\n",
      "|       8|       4|       1|     a|     1|     [4]|          []|\n",
      "|       1|       4|       1|     a|     1|     [4]|          []|\n",
      "|       2|       4|       1|     a|     1|     [4]|          []|\n",
      "|       3|       4|       1|     a|     1|     [4]|          []|\n",
      "|       5|       5|       1|     i|     1|     [5]|      [2, 3]|\n",
      "|       2|       5|       1|     a|     1|     [5]|          []|\n",
      "|       3|       5|       1|     a|     1|     [5]|          []|\n",
      "|       6|       6|       1|     i|     1|     [6]|      [9, 7]|\n",
      "|       9|       6|       1|     a|     1|     [6]|          []|\n",
      "|       7|       6|       1|     a|     1|     [6]|          []|\n",
      "|       7|       7|       1|     i|     1|     [7]|   [8, 9, 6]|\n",
      "|       8|       7|       1|     a|     1|     [7]|          []|\n",
      "|       9|       7|       1|     a|     1|     [7]|          []|\n",
      "|       6|       7|       1|     a|     1|     [7]|          []|\n",
      "|       8|       8|       1|     i|     1|     [8]|[1, 4, 9, 7]|\n",
      "|       1|       8|       1|     a|     1|     [8]|          []|\n",
      "|       4|       8|       1|     a|     1|     [8]|          []|\n",
      "|       9|       8|       1|     a|     1|     [8]|          []|\n",
      "|       7|       8|       1|     a|     1|     [8]|          []|\n",
      "|       9|       9|       1|     i|     1|     [9]|   [8, 6, 7]|\n",
      "|       8|       9|       1|     a|     1|     [9]|          []|\n",
      "|       6|       9|       1|     a|     1|     [9]|          []|\n",
      "|       7|       9|       1|     a|     1|     [9]|          []|\n",
      "|       4|       4|       2|     a|     1|     [2]|          []|\n",
      "|       4|       4|       1|     a|     1|     [3]|      [1, 3]|\n",
      "|       4|       4|       2|     a|     1|     [2]|          []|\n",
      "|       4|       4|       5|     a|     1|     [2]|          []|\n",
      "|       4|       4|       1|     a|     1|     [3]|      [1, 3]|\n",
      "|       4|       4|       1|     a|     1|     [5]|         [5]|\n",
      "|       4|       4|       1|     a|     1|     [2]|         [2]|\n",
      "+--------+--------+--------+------+------+--------+------------+\n",
      "\n",
      "root\n",
      " |-- targetId: integer (nullable = false)\n",
      " |-- sourceId: integer (nullable = false)\n",
      " |-- distance: integer (nullable = false)\n",
      " |-- status: string (nullable = false)\n",
      " |-- weight: integer (nullable = false)\n",
      " |-- pathInfo: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- adjList: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "fields = [\n",
    "    StructField('targetId', IntegerType(), False),\n",
    "    StructField('sourceId', IntegerType(), False),\n",
    "    StructField('distance', IntegerType(), False),\n",
    "    StructField('status', StringType(), False), \n",
    "    StructField('weight', IntegerType(), False),\n",
    "    StructField('pathInfo', ArrayType(IntegerType(), True), False), \n",
    "    StructField('adjList', ArrayType(IntegerType(), True), False)]\n",
    "schema = StructType(fields)\n",
    "\n",
    "temp1 = pos_map1_test.map(lambda x: (int(x[0]),int(x[1][0]),int(x[1][1]),str(x[1][2]),int(x[1][3]),[int(s) for s in x[1][4]],[int(s) for s in x[1][5]])).collect()\n",
    "temp2 = sc.parallelize(temp1)\n",
    "df = spark.createDataFrame(temp2, schema)\n",
    "df.show(100)\n",
    "\n",
    "print(df.printSchema())\n",
    "df.registerTempTable(\"Stage1_pre_reduce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+------+------+--------+------------+\n",
      "|targetId|sourceId|distance|status|weight|pathInfo|     adjList|\n",
      "+--------+--------+--------+------+------+--------+------------+\n",
      "|       1|       1|       1|     i|     1|     [1]|[8, 2, 3, 4]|\n",
      "|       1|       2|       1|     a|     1|     [2]|          []|\n",
      "|       1|       3|       1|     a|     1|     [3]|          []|\n",
      "|       1|       4|       1|     a|     1|     [4]|          []|\n",
      "|       1|       8|       1|     a|     1|     [8]|          []|\n",
      "|       2|       1|       1|     a|     1|     [1]|          []|\n",
      "|       2|       2|       1|     i|     1|     [2]|   [1, 4, 5]|\n",
      "|       2|       4|       1|     a|     1|     [4]|          []|\n",
      "|       2|       5|       1|     a|     1|     [5]|          []|\n",
      "|       3|       1|       1|     a|     1|     [1]|          []|\n",
      "|       3|       3|       1|     i|     1|     [3]|   [1, 4, 5]|\n",
      "|       3|       4|       1|     a|     1|     [4]|          []|\n",
      "|       3|       5|       1|     a|     1|     [5]|          []|\n",
      "|       4|       1|       1|     a|     1|     [1]|          []|\n",
      "|       4|       2|       1|     a|     1|     [2]|          []|\n",
      "|       4|       3|       1|     a|     1|     [3]|          []|\n",
      "|       4|       4|       1|     a|     1|     [3]|      [1, 3]|\n",
      "|       4|       4|       1|     i|     1|     [4]|[8, 1, 2, 3]|\n",
      "|       4|       4|       5|     a|     1|     [2]|          []|\n",
      "|       4|       4|       1|     a|     1|     [2]|         [2]|\n",
      "+--------+--------+--------+------+------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "SELECT\n",
    "  targetId,\n",
    "  sourceId,\n",
    "  distance,\n",
    "  status,\n",
    "  weight,\n",
    "  pathInfo,\n",
    "  adjList\n",
    "FROM\n",
    "  Stage1_pre_reduce\n",
    "ORDER BY targetId, sourceId\n",
    "\"\"\"\n",
    "df_ = sqlContext.sql(q)\n",
    "df_.registerTempTable(\"Sorted\")\n",
    "df_.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT\n",
    "  targetId,\n",
    "  sourceId,\n",
    "  distance,\n",
    "  MIN(distance) OVER (PARTITION BY sourceId ORDER BY targetId) as min_distance,\n",
    "  status,\n",
    "  weight,\n",
    "  pathInfo,\n",
    "  adjList\n",
    "FROM\n",
    "  Stage1_pre_reduce\n",
    "\"\"\"\n",
    "sqlContext.sql(q).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Result and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare above result with that of non-parallelized algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
